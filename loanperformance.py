# -*- coding: utf-8 -*-
"""Loanperformance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GcgdonsMDkNLMcPEEVmaz8QlJo1JtANX
"""

from google.colab import files

# Upload the CSV files
uploaded = files.upload()

import pandas as pd

# Load the uploaded CSV files into pandas DataFrames
train_payment_data = pd.read_csv('train_payment_data.csv')
train_loan_data = pd.read_csv('train_loan_data.csv')
test_loan_data = pd.read_csv('test_loan_data.csv')

# code to show the table for pd.read_csv('train_payment_data.csv')


train_payment_data

print(train_payment_data.shape)

# pd.read_csv('train_loan_data.csv')

train_loan_data

print(train_loan_data.shape)

#code to show the data types


print(train_loan_data.dtypes)

print(train_payment_data.dtypes)

# code to check for ,missing number


print(train_loan_data.isnull().sum())

print(train_payment_data.isnull().sum())

"""**Data Cleaning and Pre-processing**"""

# Create a copy of the original DataFrames to avoid modifying them directly
train_loan_data_cleaned = train_loan_data.copy()
train_payment_data_cleaned = train_payment_data.copy()

# Remove columns with string data in train_payment_data_cleaned
for column in train_payment_data_cleaned.columns:
  if train_payment_data_cleaned[column].dtype == object:
    train_payment_data_cleaned = train_payment_data_cleaned.drop(column, axis=1)

# Remove columns with string data in train_loan_data_cleaned, excluding categorical columns
categorical_columns = ['paid_late']  # Replace with your actual categorical columns

for column in train_loan_data_cleaned.columns:
  if train_loan_data_cleaned[column].dtype == object and column not in categorical_columns:
    train_loan_data_cleaned = train_loan_data_cleaned.drop(column, axis=1)


# Now train_loan_data_cleaned and train_payment_data_cleaned contain only numerical data
# (excluding specified categorical columns in train_loan_data_cleaned).

train_loan_data_cleaned

train_payment_data_cleaned

# Replace missing values in train_loan_data_cleaned with the mean
for column in train_loan_data_cleaned.columns:
    if train_loan_data_cleaned[column].isnull().any():
        train_loan_data_cleaned[column].fillna(train_loan_data_cleaned[column].mean(), inplace=True)

# Replace missing values in train_payment_data_cleaned with the mean
for column in train_payment_data_cleaned.columns:
    if train_payment_data_cleaned[column].isnull().any():
        train_payment_data_cleaned[column].fillna(train_payment_data_cleaned[column].mean(), inplace=True)

print(train_loan_data_cleaned.isnull().sum())
print(train_payment_data_cleaned.isnull().sum())

train_loan_data_cleaned

train_payment_data_cleaned

from sklearn.preprocessing import LabelEncoder

# Assume train_loan_data_cleaned is your DataFrame containing the 'paid_late' column
label_encoder = LabelEncoder()
train_loan_data_cleaned['paid_late'] = label_encoder.fit_transform(train_loan_data_cleaned['paid_late'])

train_loan_data_cleaned

"""**saving the cleaned data into CSV file**"""

train_loan_data_cleaned.to_csv('train_loan_data_cleaned.csv', index=False)

train_payment_data_cleaned.to_csv('train_payment_data_cleaned.csv', index=False)

"""**Merging the train_payment_data and train_loan_data on loan_id to create a unified dataset for training machine learning models**
- we can combine them based on the loan_id column, which is the common identifier in both datasets.
"""

import pandas as pd

# Load datasets
train_loan_data_cleaned = pd.read_csv('train_loan_data_cleaned.csv')
train_payment_data_cleaned = pd.read_csv('train_payment_data_cleaned.csv')

# Aggregate payment data if necessary (e.g., sum of payments per loan_id)
payment_agg = train_payment_data_cleaned.groupby('loan_id').agg({
    'amount': ['sum', 'count']  # Sum of total payments and count of payments
}).reset_index()

# Rename columns after aggregation
payment_agg.columns = ['loan_id', 'total_paid', 'payment_count']

# Merge the aggregated payment data with the loan data on loan_id
merged_data = pd.merge(train_loan_data_cleaned, payment_agg, on='loan_id', how='left')

# Fill missing values (if a loan_id has no payment records, we fill with 0)
merged_data.fillna(0, inplace=True)

# Inspect the merged data
print(merged_data.head())

# After this, you can use merged_data for further preprocessing (like encoding, scaling, etc.) before applying it to a machine learning model

merged_data

# Check for basic statistics of numerical features
print(merged_data.describe())

# Check for unique values in categorical features (if any)
for column in merged_data.columns:
  if merged_data[column].dtype == object:
    print(f"Unique values in {column}: {merged_data[column].unique()}")

# Check for correlations between features
correlation_matrix = merged_data.corr()
print(correlation_matrix)

# You can also visualize the correlation matrix using seaborn
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

print(train_payment_data_cleaned.dtypes)

print(train_payment_data_cleaned.shape)

print(train_loan_data_cleaned.dtypes)

print(train_loan_data_cleaned.shape)

print(merged_data.dtypes)

#to check the number of columns or features added to train_loan_data_cleaned from the payment

num_features_added = len(merged_data.columns) - len(train_loan_data_cleaned.columns)
print(f"Number of features added from payment data: {num_features_added}")

"""to check the name of features addeded to the merged data of

**generate the name of the features added train_loan_data_cleaned**
"""

added_features = [col for col in merged_data.columns if col not in train_loan_data_cleaned.columns]

print("Features added from payment data:", added_features)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(merged_data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Merged Data')
plt.show()

merged_data.to_csv('merged_data.csv', index=False)

#code to chek for the dimmesnion of the above data

print("Dimensions of merged_data:")
print(merged_data.shape)

import pandas as pd

# Load your dataset (assuming it has already been cleaned and merged)
df = pd.read_csv('merged_data.csv')

# List of columns to drop
columns_to_remove = ['loan_id', 'business_id', 'credit_officer_id', 'application_number',
                     'applying_for_loan_number', 'loan_number']

# Drop the unnecessary columns
df_cleaned = df.drop(columns=columns_to_remove)

# Preview the cleaned dataset
print(df_cleaned.head())

# Check for any missing values (Optional)
print(df_cleaned.isnull().sum())

# Optionally, fill missing values (if necessary)
df_cleaned.fillna(0, inplace=True)

# Final cleaned dataset ready for further analysis or model preparation
print(df_cleaned.shape)

df_cleaned

print(df_cleaned.shape)

# Save the cleaned data for ML to a CSV file
df_cleaned.to_csv('data_for_ml.csv', index=False)

print("Cleaned data saved to 'data_for_ml.csv'")

"""**Machine Learning Prediction**"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the cleaned data
df_cleaned = pd.read_csv('data_for_ml.csv')

# Separate features and target
X = df_cleaned.drop(columns=['paid_late'])  # Features
y = df_cleaned['paid_late']  # Target

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the feature data
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for easier handling
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Add the target variable back to the scaled DataFrame
df_normalized = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)

# Save the normalized data to CSV
df_normalized.to_csv('normalized_data_for_ml.csv', index=False)

print("Normalized data saved to 'normalized_data_for_ml.csv'")

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import LabelEncoder

# Load the cleaned data
df_normalized = pd.read_csv('normalized_data_for_ml.csv')

# Encode the target variable if necessary (assuming 'paid_late' is categorical)
label_encoder = LabelEncoder()
df_normalized['paid_late'] = label_encoder.fit_transform(df_normalized['paid_late'])

# Separate features and target
X = df_normalized.drop(columns=['paid_late'])  # Features
y = df_normalized['paid_late']  # Target

# Split the data into training and validation sets (80% training, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the training set
y_train_pred = model.predict(X_train)

# Predict on the validation set
y_val_pred = model.predict(X_val)
y_val_pred_proba = model.predict_proba(X_val)[:, 1]  # Probability for the positive class

# Evaluate the model for training set
train_accuracy = accuracy_score(y_train, y_train_pred)

# Evaluate the model for validation set
val_accuracy = accuracy_score(y_val, y_val_pred)
conf_matrix = confusion_matrix(y_val, y_val_pred)
class_report = classification_report(y_val, y_val_pred)

# Print out the accuracies
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Paid Late', 'Paid Late'], yticklabels=['Not Paid Late', 'Paid Late'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Plotting feature importances
features = X.columns
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 6))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y_val, y_val_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Save the trained model to be used later
import joblib
joblib.dump(model, 'loan_default_prediction_model.pkl')

# prompt: explain what the confusion matrix did above

# The confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.
# It allows visualization of the performance of an algorithm.
# It displays the number of correct and incorrect predictions broken down by each class.
# The confusion matrix in the above code was created to evaluate the performance of the Random Forest Classifier on the validation set.


# In the context of the code provided:
#   - True Positives (TP): The model correctly predicted that a loan would be paid late (paid_late = 1).
#   - True Negatives (TN): The model correctly predicted that a loan would not be paid late (paid_late = 0).
#   - False Positives (FP): The model incorrectly predicted that a loan would be paid late when it actually was not (paid_late = 0). Also known as a Type I error.
#   - False Negatives (FN): The model incorrectly predicted that a loan would not be paid late when it actually was (paid_late = 1). Also known as a Type II error.


# The confusion matrix is useful for understanding the different types of errors that the model is making.
# For example, a high number of false positives might mean that the model is too sensitive and is predicting paid_late too often when it is not really the case.
# On the other hand, a high number of false negatives might indicate that the model is not sensitive enough and is missing instances of loans that are likely to be paid late.

# By analyzing the confusion matrix, you can identify areas where the model needs to be improved.

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import LabelEncoder

# Load the cleaned data
df_normalized = pd.read_csv('normalized_data_for_ml.csv')

# Encode the target variable if necessary (assuming 'paid_late' is categorical)
label_encoder = LabelEncoder()
df_normalized['paid_late'] = label_encoder.fit_transform(df_normalized['paid_late'])

# Separate features and target
X = df_normalized.drop(columns=['paid_late'])  # Features
y = df_normalized['paid_late']  # Target

# Initialize the Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Perform k-fold cross-validation (e.g., 5 folds)
cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# Print the cross-validation scores
print("Cross-Validation Scores:")
print(cv_scores)
print(f"Mean Accuracy: {cv_scores.mean() * 100:.2f}%")
print(f"Standard Deviation: {cv_scores.std() * 100:.2f}%")

# Optionally fit the model on the entire dataset after cross-validation
model.fit(X, y)

# Predict on the validation set
y_pred = model.predict(X)

# Calculate the predicted probabilities for the positive class
y_pred_proba = model.predict_proba(X)[:, 1]  # Probability for the positive class

# Evaluate the model
accuracy = accuracy_score(y, y_pred)
conf_matrix = confusion_matrix(y, y_pred)
class_report = classification_report(y, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Paid Late', 'Paid Late'],
            yticklabels=['Not Paid Late', 'Paid Late'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Plotting feature importances
features = X.columns
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 6))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Save the trained model to be used later
import joblib
joblib.dump(model, 'loan_default_prediction_model1.pkl')

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='paid_late', data=merged_data)
plt.title('Count of Loans Paid Late vs. On Time')
plt.show()

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.exceptions import ConvergenceWarning
import warnings

# Load your dataset
df_normalized = pd.read_csv('normalized_data_for_ml.csv')

# Separate features and target
X = df_normalized.drop(columns=['paid_late'])
y = df_normalized['paid_late']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize model and success flag
model = RandomForestClassifier(n_estimators=100, random_state=42)
success = False
max_retries = 5  # Maximum retries before stopping
retries = 0      # Initialize retry counter

# Suppress convergence warnings for demonstration purposes
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Define the training loop with hyperparameter adjustment and convergence check
while not success and retries < max_retries:
    try:
        # Train the model
        model.fit(X_train, y_train)
        success = True
    except ConvergenceWarning as e:
        print("Convergence warning occurred:", e)
        # Adjust hyperparameters (for example, increasing the number of trees)
        model.set_params(n_estimators=model.n_estimators + 50)
        retries += 1

    # Check if the model overfits (simple check using training vs test accuracy)
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)

    if train_accuracy > 0.99 and test_accuracy < 0.7:
        print("Model overfitting detected. Stopping training.")
        break

# If training was successful, make predictions
if success:
    # Predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Test Accuracy: {accuracy * 100:.2f}%")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
else:
    print("Training was not successful after multiple attempts.")

"""**Explanation**:
- Training Loop: The model is trained in a loop. If a ConvergenceWarning is raised, it retries the training with adjusted hyperparameters (increasing the number of trees in RandomForestClassifier as an example).

- Overfitting Check: A simple check for overfitting is added. If training accuracy is extremely high (greater than 99%), but testing accuracy is relatively low (less than 70%), it breaks the loop, assuming the model has overfitted to the training data.

- Hyperparameter Adjustment: In case of issues during training (e.g., convergence problems), the model adjusts the number of estimators (trees) in the random forest and retries the training.

- Prediction: Once the model is successfully trained, predictions are made on the test set, and the model's performance is evaluated using accuracy and a classification report.

**To check the  feature importance**
"""

# Get feature importance
feature_importances = model.feature_importances_
features = X.columns

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plotting feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.show()

"""**Training and validating the model (or making predictions) against the 2,000 loans in the test data.**"""

test_loan_data

print(test_loan_data.shape)

print(test_loan_data.dtypes)

# check missing number of the test data

print(test_loan_data.isnull().sum())

# Create a copy of the original DataFrame to avoid modifying it directly
test_loan_data_cleaned = test_loan_data.copy()

# Specify the categorical columns to keep
categorical_columns = ['paid_late']  # Replace with your actual categorical columns

# Remove columns with string data and the 'dismissal_description' column, excluding categorical columns
for column in test_loan_data_cleaned.columns:
    if (test_loan_data_cleaned[column].dtype == object and
        column not in categorical_columns and
        column != 'dismissal_description'):
        test_loan_data_cleaned = test_loan_data_cleaned.drop(column, axis=1)

# Display the cleaned DataFrame
print(test_loan_data_cleaned.head())

test_loan_data_cleaned

# Create a copy of the original DataFrame to avoid modifying it directly
test_loan_data_cleaned = test_loan_data.copy()

# Specify the categorical columns to keep
categorical_columns = ['paid_late']  # Replace with your actual categorical columns

# Remove 'dismissal_description' column if it exists
if 'dismissal_description' in test_loan_data_cleaned.columns:
    test_loan_data_cleaned = test_loan_data_cleaned.drop('dismissal_description', axis=1)

# Remove columns with string data, excluding categorical columns
for column in test_loan_data_cleaned.columns:
    if (test_loan_data_cleaned[column].dtype == object and
        column not in categorical_columns):
        test_loan_data_cleaned = test_loan_data_cleaned.drop(column, axis=1)

# Display the cleaned DataFrame
print(test_loan_data_cleaned.head())

test_loan_data_cleaned

# dimmension

print(test_loan_data_cleaned.shape)

#  datatypes

print(test_loan_data_cleaned.dtypes)

#check for missing number

print(test_loan_data_cleaned.isnull().sum())

# place the missing number

# Replace missing values in test_loan_data_cleaned with the mean
for column in test_loan_data_cleaned.columns:
    if test_loan_data_cleaned[column].isnull().any():
        test_loan_data_cleaned[column].fillna(test_loan_data_cleaned[column].mean(), inplace=True)

print(test_loan_data_cleaned.isnull().sum())

test_loan_data_cleaned

from sklearn.preprocessing import LabelEncoder

# Assume train_loan_data_cleaned is your DataFrame containing the 'paid_late' column
label_encoder = LabelEncoder()
test_loan_data_cleaned['paid_late'] = label_encoder.fit_transform(test_loan_data_cleaned['paid_late'])

test_loan_data_cleaned

test_loan_data_cleaned.to_csv('test_loan_data_cleaned.csv', index=False)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(test_loan_data_cleaned.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of test_loan_data_cleaned')
plt.show()

import pandas as pd
test_loan_data_cleaned = pd.read_csv('test_loan_data_cleaned.csv')
print(test_loan_data_cleaned.shape)

"""**Merge the test data to Payment data**"""

import pandas as pd

# Load datasets
test_loan_data_cleaned = pd.read_csv('test_loan_data_cleaned.csv')
train_payment_data_cleaned = pd.read_csv('train_payment_data_cleaned.csv')

# Aggregate payment data if necessary (e.g., sum of payments per loan_id)
payment_agg = train_payment_data_cleaned.groupby('loan_id').agg({
    'amount': ['sum', 'count']  # Sum of total payments and count of payments
}).reset_index()

# Rename columns after aggregation
payment_agg.columns = ['loan_id', 'total_paid', 'payment_count']

# Merge the aggregated payment data with the loan data on loan_id
merged_data1 = pd.merge(test_loan_data_cleaned, payment_agg, on='loan_id', how='left')

# Fill missing values (if a loan_id has no payment records, we fill with 0)
merged_data1.fillna(0, inplace=True)

# Inspect the merged data
print(merged_data1.head())

# After this, you can use merged_data for further preprocessing (like encoding, scaling, etc.) before applying it to a machine learning model

merged_data1

print("Dimensions of merged_data1:")
print(merged_data1.shape)

merged_data1.to_csv('merged_data1.csv', index=False)

import pandas as pd

# Load your dataset (assuming it has already been cleaned and merged)
df = pd.read_csv('merged_data1.csv')

# List of columns to drop
columns_to_remove = ['loan_id', 'business_id', 'credit_officer_id', 'application_number',
                     'applying_for_loan_number', 'loan_number']

# Drop the unnecessary columns
df_cleaned1 = df.drop(columns=columns_to_remove)

# Preview the cleaned dataset
print(df_cleaned1.head())

# Check for any missing values (Optional)
print(df_cleaned1.isnull().sum())

# Optionally, fill missing values (if necessary)
df_cleaned1.fillna(0, inplace=True)

# Final cleaned dataset ready for further analysis or model preparation
print(df_cleaned1.shape)

df_cleaned1

print(df_cleaned1.shape)

# Save df_cleaned1 to a CSV file
df_cleaned1.to_csv('data_for_ml1.csv', index=False)

"""***Normalization***"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the cleaned data
df_cleaned = pd.read_csv('data_for_ml1.csv')

# Separate features and target
X = df_cleaned.drop(columns=['paid_late'])  # Features
y = df_cleaned['paid_late']  # Target

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the feature data
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for easier handling
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Add the target variable back to the scaled DataFrame
df_normalized1 = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)

# Save the normalized data to CSV
df_normalized1.to_csv('normalized_data_for_ml1.csv', index=False)

print("Normalized data saved to 'normalized_data_for_ml1.csv'")

# Import necessary libraries
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

# Load the trained model
model = joblib.load('loan_default_prediction_model.pkl')

# Load the test data (already normalized)
test_data = pd.read_csv('normalized_data_for_ml1.csv')

# Separate features and target (assuming 'paid_late' is the target)
X_test = test_data.drop(columns=['paid_late'])  # Features
y_test = test_data['paid_late']  # True labels

# Make predictions using the loaded model
predictions = model.predict(X_test)

# Evaluate the model performance
accuracy = accuracy_score(y_test, predictions)
conf_matrix = confusion_matrix(y_test, predictions)
class_report = classification_report(y_test, predictions)
auc_score = roc_auc_score(y_test, predictions)

# Print evaluation metrics
print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)
print(f"AUC Score: {auc_score:.2f}")

# Add predictions to the test data for future analysis or evaluation
test_data['predicted_paid_late'] = predictions

# Save the predictions to a new CSV file
test_data.to_csv('test_loan_predictions.csv', index=False)

print("Predictions saved to 'test_loan_predictions.csv'")

# Import necessary libraries
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.exceptions import ConvergenceWarning
import warnings

# Load the test data (already normalized)
test_data = pd.read_csv('normalized_data_for_ml1.csv')

# Separate features and target (assuming 'paid_late' is the target)
X_test = test_data.drop(columns=['paid_late'])  # Features
y_test = test_data['paid_late']  # True labels

# Load the trained model
try:
    model = joblib.load('loan_default_prediction_model.pkl')
except FileNotFoundError:
    print("Trained model not found. Training a new model instead.")
    # If the model isn't found, let's retrain it
    success = False
    max_retries = 5
    retries = 0

    # Load training data (assuming it's in the same format as test data)
    train_data = pd.read_csv('normalized_data_for_ml.csv')
    X_train = train_data.drop(columns=['paid_late'])  # Features
    y_train = train_data['paid_late']  # Target

    model = RandomForestClassifier(n_estimators=100, random_state=42)

    # Suppress convergence warnings for demonstration purposes
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    # Retry training in a loop with hyperparameter adjustment
    while not success and retries < max_retries:
        try:
            model.fit(X_train, y_train)
            success = True
        except ConvergenceWarning:
            print("Convergence warning encountered. Adjusting hyperparameters...")
            model.set_params(n_estimators=model.n_estimators + 50)  # Example hyperparameter adjustment
            retries += 1

        # Check for overfitting using training vs. test accuracy (using cross-validation may be better)
        train_accuracy = model.score(X_train, y_train)
        test_accuracy = model.score(X_test, y_test)

        if train_accuracy > 0.99 and test_accuracy < 0.7:
            print("Overfitting detected. Stopping training.")
            break

    # Save the newly trained model to file
    if success:
        joblib.dump(model, 'loan_default_prediction_model.pkl')
        print("Model trained and saved successfully.")
    else:
        print("Training was not successful after multiple attempts.")
        exit(1)

# If the model is already trained or successfully trained, proceed to prediction
predictions = model.predict(X_test)

# Evaluate the model performance
accuracy = accuracy_score(y_test, predictions)
conf_matrix = confusion_matrix(y_test, predictions)
class_report = classification_report(y_test, predictions)
auc_score = roc_auc_score(y_test, predictions)

# Print evaluation metrics
print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)
print(f"AUC Score: {auc_score:.2f}")

# Add predictions to the test data for future analysis or evaluation
test_data['predicted_paid_late'] = predictions

# Save the predictions to a new CSV file
test_data.to_csv('test_loan_predictions.csv1', index=False)

print("Predictions saved to 'test_loan_predictions.csv1")

"""
- The code first tries to load a pre-trained model (loan_default_prediction_model.pkl). If the model is not found, it retrains the model using a loop similar to your provided structure.
Retry Logic for Training:

- The model training is wrapped in a loop that retries up to max_retries times if a ConvergenceWarning occurs. Each retry increases the number of trees (n_estimators) in the RandomForestClassifier by 50.
Overfitting Check:

- After each training attempt, the code checks if the model is overfitting by comparing training accuracy to test accuracy (a simple heuristic). If overfitting is detected, the training stops.
Prediction:

- If the model is successfully trained or loaded, it proceeds to make predictions on the test set and evaluates the modelâ€™s performance.
"""

# Create a DataFrame with actual and predicted values
comparison_df = pd.DataFrame({
    'Actual Paid Late': y_test,
    'Predicted Paid Late': test_data['predicted_paid_late']
})

# Display the comparison table
print(comparison_df.head(10))  # Display the first 10 rows for brevity

# Optionally, save the comparison to a CSV file for further analysis
comparison_df.to_csv('paid_late_comparison.csv', index=False)
print("Comparison saved to 'paid_late_comparison.csv'")

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Paid Late', 'Paid Late'],
            yticklabels=['Not Paid Late', 'Paid Late'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = {:.2f})'.format(auc_score))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Assuming the model has a feature_importances_ attribute (like tree-based models)
if hasattr(model, 'feature_importances_'):
    feature_importances = model.feature_importances_
    features = X_test.columns

    # Create a DataFrame for feature importance
    feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')
    plt.title('Feature Importance')
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create a DataFrame to count occurrences of true vs predicted values
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': test_data['predicted_paid_late']
})

# Melt the DataFrame for easier plotting
comparison_melted = comparison.melt(var_name='Type', value_name='Value')

# Plotting
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=comparison_melted, x='Value', hue='Type', palette='pastel')

# Annotate with count values
for p in ax.patches:
    height = p.get_height()
    count = int(height)  # Get the count value directly from the height
    ax.annotate(f'{count}',
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom')

plt.title('Comparison of Actual vs Predicted Values of Paid Late')
plt.xlabel('Value (0 = Not Paid Late, 1 = Paid Late)')
plt.ylabel('Count')
plt.legend(title='Type', labels=['Actual', 'Predicted'])
plt.grid(axis='y')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create a DataFrame to count occurrences of true vs predicted values
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': test_data['predicted_paid_late']
})

# Calculate the count of each category for both actual and predicted
actual_counts = comparison['Actual'].value_counts()
predicted_counts = comparison['Predicted'].value_counts()

# Calculate the percentages for actual values
actual_paid_late_percentage = (actual_counts[1] / actual_counts.sum()) * 100
actual_not_paid_late_percentage = (actual_counts[0] / actual_counts.sum()) * 100

# Calculate the percentages for predicted values
predicted_paid_late_percentage = (predicted_counts[1] / predicted_counts.sum()) * 100
predicted_not_paid_late_percentage = (predicted_counts[0] / predicted_counts.sum()) * 100

# Print out the percentages
print(f"Actual Percentage of People who Paid Late: {actual_paid_late_percentage:.2f}%")
print(f"Actual Percentage of People who Paid on Time: {actual_not_paid_late_percentage:.2f}%")
print(f"Predicted Percentage of People who Paid Late: {predicted_paid_late_percentage:.2f}%")
print(f"Predicted Percentage of People who Paid on Time: {predicted_not_paid_late_percentage:.2f}%")

# Melt the DataFrame for easier plotting
comparison_melted = comparison.melt(var_name='Type', value_name='Value')

# Plotting
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=comparison_melted, x='Value', hue='Type', palette='pastel')

# Annotate with count values
for p in ax.patches:
    height = p.get_height()
    count = int(height)  # Get the count value directly from the height
    ax.annotate(f'{count}',
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom')

plt.title('Comparison of Actual vs Predicted Values of Paid Late')
plt.xlabel('Value (0 = Not Paid Late, 1 = Paid Late)')
plt.ylabel('Count')
plt.legend(title='Type', labels=['Actual', 'Predicted'])
plt.grid(axis='y')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, test_data['predicted_paid_late'])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Paid Late', 'Paid Late'],
            yticklabels=['Not Paid Late', 'Paid Late'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""

# Assuming 'conf_matrix' is the confusion matrix from your model evaluation

# Explanation of the confusion matrix plot:

# - **True Positives (TP):** The number of instances correctly predicted as 'Paid Late'.
#   - Located in the bottom-right cell of the matrix.
# - **True Negatives (TN):** The number of instances correctly predicted as 'Not Paid Late'.
#   - Located in the top-left cell of the matrix.
# - **False Positives (FP):** The number of instances incorrectly predicted as 'Paid Late' (Type I error).
#   - Also known as a "false alarm".
#   - Located in the top-right cell of the matrix.
# - **False Negatives (FN):** The number of instances incorrectly predicted as 'Not Paid Late' (Type II error).
#   - Also known as a "missed detection".
#   - Located in the bottom-left cell of the matrix.

# The plot helps visualize the performance of a classification model by showing how many
# predictions were correct and incorrect for each class (Paid Late and Not Paid Late).
# It is useful to understand the types of errors the model is making.

# Example interpretation:
# If the top-left cell (TN) has a high value, it means the model accurately predicts
# "Not Paid Late" cases. If the bottom-right cell (TP) has a high value, it means
# the model accurately predicts "Paid Late" cases.
# However, if the top-right cell (FP) or bottom-left cell (FN) have high values,
# it indicates that the model is making a significant number of errors, and the model's
# accuracy might need improvement.

# The 'heatmap' visualization with annotations makes the numbers more easily understandable.
# The color intensity indicates the magnitude of the values in the cells.
"""

plt.figure(figsize=(10, 6))
sns.kdeplot(data=comparison, x='Actual', fill=True, label='Actual', alpha=0.5)
sns.kdeplot(data=comparison, x='Predicted', fill=True, label='Predicted', alpha=0.5)
plt.title('Density Plot of Actual vs Predicted Paid Late')
plt.xlabel('Paid Late Status')
plt.ylabel('Density')
plt.legend()
plt.show()

pip install matplotlib seaborn

import pandas as pd

# Assuming y_test contains actual values and predictions contains predicted values
# Here we load the actual values and predicted values into a DataFrame
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': test_data['predicted_paid_late']
})

# Calculate percentage error
# To avoid division by zero, we will add a small value (e.g., 1e-9) to the actual values
comparison['Percentage Error'] = ((comparison['Predicted'] - comparison['Actual']) / (comparison['Actual'] + 1e-9)) * 100

# Display the results
print(comparison[['Actual', 'Predicted', 'Percentage Error']])

"""**Actual,Predicted,Percentage Error table**"""

import pandas as pd

# Assuming y_test contains actual values and predictions contains predicted values
# Here we load the actual values and predicted values into a DataFrame
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': test_data['predicted_paid_late']
})

# Calculate percentage error
# To avoid division by zero, we will add a small value (e.g., 1e-9) to the actual values
comparison['Percentage Error'] = ((comparison['Predicted'] - comparison['Actual']) / (comparison['Actual'] + 1e-9)) * 100

# Save the DataFrame to a CSV file
comparison.to_csv('predictions_comparison.csv', index=False)

# Confirm that the file has been saved
print("Comparison saved to 'predictions_comparison.csv'.")

"""
# It's generally expected that the performance on the test data  will be slightly lower than the performance on the training or validation data.

why:

# * Overfitting: The model might have learned the training data too
well, including its noise and random variations. This can lead to
lower accuracy when applied to new, unseen data.
# * Generalization: The goal is for the model to generalize well to
new data, not just memorize the training data. Test data helps us
assess how well the model can generalize.


# In the context of our work, we've already trained and evaluated the
 model using cross-validation and also on a validation set. The
 performance you see on the test data is a more realistic measure of
how the model will perform in the real world when we deploy it to
 make predictions on new loan data.

# If the test performance is significantly worse than the training
or validation performance, it could indicate that the model is
overfitting. You might need to consider strategies like:

# * Reducing model complexity (e.g., using a simpler model or fewer
features)
# * Regularization techniques (e.g., L1 or L2 regularization)
# * Increasing training data size
# * Hyperparameter tuning (e.g., using grid search or random search)

# However, keep in mind that a slight drop in accuracy on the test
 data compared to the training data is usually acceptable and
expected."""